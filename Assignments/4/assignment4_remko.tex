%
% File lfd1617.tex
%
%% Based on the style files for EACL-2017
%% Based on the style files for ACL-2016
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}

%%%% LEAVE THIS IN
\eaclfinalcopy


\newcommand\BibTeX{B{\sc ib}\TeX}



\title{Learning from Data - Assignment 4 - Perceptrons and Word Embeddings}

\author{Remko Boschker \\
  master student of information science at the Rijks Universiteit Groningen \\
  {\tt s1282603, r.boschker@student.rug.nl} }

\date{}

\begin{document}
\maketitle
\begin{abstract}
In the following I use the word2vec toolkit and evaluate some of my findings. I also use word embeddings as features for a perceptron classifier. I develop a baseline classifier for comparison and tune the parameters of the algorithm. The perceptron performs a binary classification at an f-1 and accuracy score of 0.91 and a six-way classification at scores of 0.80. The parameter tuning shows that the parameters settings are interrelated and I found no clear optimum.  Finally I look into the misclassifications and find items are often mis-categorised as organisations possibly because they occur in many contexts and a perceptron is a linear classifier.
\end{abstract}

\section{Introduction}
The use of word embeddings in natural language processing tasks has become successful due to the availability of large amounts of data, efficient pre-training of representations and its effectiveness. In the following experiments I first use the word2vec toolkit to look at words with similar embeddings and to find analogies between sets of words. Secondly I combine word embeddings as features with a perceptron classifier to categorise named entities.

\section{Word2vec}

I use the word2vec toolkit to experiment with word embeddings. The toolkit uses the \emph{GoogleNews-vectors-negative300bin} pre-trained word embedding. First I use the distance tool with five words that have strong connotations or more than one meaning. I discuss any unexpected results and go a little into word sense ambiguity. Next I use the analogy tool to evaluate word2vec ability to capture analogies.

The distance tool lists the forty words that are closest to the word entered in terms of the cosine distance between vector representations of the words embeddings. Below I list and discuss the results for five words.

\paragraph{Java}
I expected there to be words related to the programming language and the Indonesian island. And there were for instance \emph{Jakarta} and \emph{J2EE}. I did not expect to find \emph{Jumpin Juice}, a coffee and juice bar franchise. The franchise advertises a lot with the word \emph{Java} as a reference to coffee.

\paragraph{Python}
For the word \emph{python} I expected to also find word related to the snake and to the programming language. But I only found names of snakes and one or two other reptiles.

\paragraph{Dutch}
I expected to find words typically associated with being \emph{Dutch} (at least in an American news corpus) such as cheese and tulips and coffeeshops. But I found mostly other nationalities such as \emph{Belgian, German} and \emph{Netherlands}. Also there were two Dutch newspapers \emph{Volkskrant} and \emph{Telegraaf}. Probably because they were quoted in the corpus. And a financial analyst and a Dutch spokesman for the IS UN taskforce \emph{analyst\_Pieter\_Kulsen} and emph{Edmond\_Messchaert\_spokesman}.

\paragraph{economy}
Besides the expected terminlogy such as \emph{job\_rate, gross\_domestic\_product, recession}, I found an apparently very common  spelling error \emph{theeconomy}, a large investment firm \emph{EFG\_Hermes\_Ziada} and the mysterious \emph{Wehbey}, the last name of various government officials in the field of economics in Central and South America.

\paragraph{New York}
For \emph{New York} I found the five boroughs \emph{Manhattan, Brooklyn, Queens, Bronx, Staten Island} and \emph{Long Island} as expected. I also found baseball related terms \emph{RBI\_ARodriguez, STRIKEOUTS\_Mussina, STOLEN\_BASES\_Reyes} to do with the baseball team the New York Yankees. Unexpectedly I also found a New York Times photographer and journalist \emph{Hiroko\_Masuike, By\_PENELOPE\_GREEN}. These are probably the result of the news based corpus used for training.
\\

In general it is to be expected that the embeddings for a particular word reflect the different meanings of a word. Whereas polysemous words for example \emph{man, mole, bank, point, roll, wave} can be expected to have similar embeddings because these are words that have multiple but related meanings. For homonymic words such as \emph{arm, bank, match, iron, bear} the different meanings are not related and different groups of related terms can be discerned.

Word sense disambiguation is the problem in natural language processing of selecting which of a words sense is the one intended in a sentence when there is more than one. Word embeddings are not a good feature for this task, because although the embeddings show words related to the different senses of a word it does not distinguish between them. And therefore given a context you might find words that are also in the embedding, but you have no way to derive the particular sense from that.

Also the word senses present in an embedding is influenced by the corpus uses for training the embedding. For instance the Google News vectors shows only words related to \emph{cookie} as a baked good such as \emph{peanut\_butter\_cookie, cupcake, oatmeal\_cookie}. The alternate 'vectors.bin' training file also shows words related to the sense of a \emph{cookie} that is set in a webbrowser such as \emph{url, server, and password}. Apparently the vectors.bin vectors were trained on a corpus containing relatively more texts about internet and web-browser related content.

I also use the analogy tool. This tool can given three term find an analogues fourth by adding the first two vectors and subtracting the third. For instance $man + king - woman = queen$. I gave it a few tries before I found some relations that worked. For instance $grass + green - roses$ did not equal $red$.
The \emph{president/prime-minister} and \emph{country} relation works well although the corpus is a bit dated.
\begin{itemize}
\item[] France Hollande Germany Schr\"{o}der
\item[] France Hollande Bolivia Chavez
\item[] Bolivia Chavez Spain Zapatero
\item[] Spain Zapatero Netherlands Balkenende
\end{itemize}

I expected \emph{sporter} and \emph{sport} to be an easy analogy. But when trying \emph{Kramer skating Armstrong}, I only found cycling in third place. As it turns out there is also a skateboarder and another ice skater named \emph{Armstrong} accounting for first and second place.

\begin{itemize}
\item[] Kramer skating Armstrong skater
\item[] Messi Soccer Nadal tennis
\item[] Nadal tennis Cruijff soccer
\end{itemize}

A third relation worked a expected, the one between a make of car and the country it was (originally) made.

\begin{itemize}
\item[] Fiat Italy Peugeot France
\item[] Citroen France Mercedes Germany
\item[] Opel Germany Volvo Sweden
\end{itemize}

\section{Perceptron}

The following experiments use a stochastic gradient descent algorithm to implement a perceptron classifier using word embeddings as features. First I look at the data, implement and evaluate a baseline system. Then I try to tune the parameters of the algorithm. Next I investigate if the classifier generalises well to classify names that are similar to ones in the training data, but are new. Lastly I evaluate misclassifications by inspecting the confusion matrix.

\subsection{Data}

\begin{table}[ht]
  \caption{category distribution for the named entity list}
  \label{tab:ne-cats}
  \begin{tabular}{ l r r }
    label & count & percentage \\
    \hline
    CARDINAL     &  5291 &  14.9\% \\
    GPE          & 11392 &  32.0\% \\
    PERSON       &  5955 &  16.7\% \\
    DATE         &  4213 &  11.9\% \\
    ORG          &  8131 &  22.8\% \\
    LOC          &   613 &   1.7\% \\
    location     & 12005 &  33.7\% \\
    non-location & 23590 &  66.3\% \\
    tot          & 35595 & 100.0\% \\
  \end{tabular}
\end{table}

The following experiments use a list of 35595 named entities labeled with a category. Table ~\ref{tab:ne-cats} lists the distribution of the categories. They also use a binary file containing a Python dictionary containing 50-dimension word vectors representing word embeddings taken from https://nlp.stanford.edu/projects/glove/.

\subsection{Baseline}

\begin{table}[ht]\footnotesize
  \caption{results for baseline systems for classification}
  \label{tab:baseline}
  \begin{tabular}{ l | r r | r r | r r }
    & \multicolumn{2}{c}{provided} & \multicolumn{2}{c}{dist} & \multicolumn{2}{c}{most} \\
    \hline
    \# & acc & f-1 & acc & f-1 & acc & f-1 \\
    2-way & 0.81 & 0.80 & 0.56 & 0.55 & 0.65 & 0.52 \\
    6-way & 0.79 & 0.79 & 0.22 & 0.22 & 0.32 & 0.16 \\
  \end{tabular}
\end{table}

The provided script appears to perform quite well, but results can be caused by a skewed distribution in the dataset. Table~\ref{tab:ne-cats} shows that this may wel be the case. In order to evaluate the performance I wrote two baseline systems. One system uses the binary distribution of a location and  a non-location labels and labels every third items as a location. Another systems labels every items as a non-location. Similarly for the six-way classification one base-line system labels roughly according to the distribution and one labels all entities as the most common label \emph{GPE}. The performance of the provided system and the baselines are listed in table~\ref{tab:baseline}. The results show that the perceptron performs well compared to the baseline systems.

\subsection{Parameter tuning}

\begin{table}[ht]\footnotesize
  \caption{results for different perceptron parameter settings}
  \label{tab:param}
  \begin{tabular}{llrrrr}
    learning   & penalty    & iter & type & f1   & acc  \\
    \hline
    constant   & none       & 5    & 2    & 0.80 & 0.81 \\
    constant   & none       & 5    & 6    & 0.79 & 0.79 \\
    constant   & none       & 10   & 2    & \textbf{0.91} & \textbf{0.91} \\
    constant   & none       & 10   & 6    & 0.77 & 0.76 \\
    constant   & none       & 25   & 2    & 0.88 & 0.88 \\
    constant   & none       & 25   & 6    & 0.76 & 0.76 \\
    \cline{2-6}
    constant   & l2         & 5    & 2    & 0.88 & 0.88 \\
    constant   & l2         & 5    & 6    & 0.76 & 0.76 \\
    constant   & l2         & 10   & 2    & 0.87 & 0.87 \\
    \cline{2-6}
    constant   & l1         & 5    & 2    & 0.88 & 0.88 \\
    constant   & l1         & 5    & 6    & 0.78 & 0.78 \\
    constant   & l1         & 10   & 2    & 0.90 & 0.90 \\
    constant   & l1         & 25   & 2    & 0.81 & 0.83 \\
    \cline{2-6}
    constant   & elasticnet & 5    & 2    & 0.78 & 0.78 \\
    constant   & elasticnet & 5    & 6    & 0.67 & 0.66 \\
    \hline
    optimal    & none       & 5    & 2    & 0.90 & 0.90 \\
    optimal    & none       & 5    & 6    & 0.79 & 0.79 \\
    optimal    & none       & 10   & 2    & 0.72 & 0.76 \\
    \cline{2-6}
    optimal    & l1         & 5    & 2    & 0.82 & 0.83 \\
    optimal    & l1         & 5    & 6    & 0.62 & 0.62 \\
    optimal    & l1         & 10   & 2    & \textbf{0.91} & \textbf{0.91} \\
    optimal    & l1         & 25   & 2    & 0.88 & 0.88 \\
    \hline
    invscaling & none       & 5    & 2    & \textbf{0.91} & \textbf{0.91} \\
    invscaling & none       & 5    & 6    & 0.78 & 0.77 \\
    invscaling & none       & 10   & 2    & 0.74 & 0.76 \\
    \cline{2-6}
    invscaling & l1         & 5    & 6    & \textbf{0.80} & \textbf{0.80} \\
    invsclaing & l1         & 5    & 2    & 0.86 & 0.86 \\
    invscaling & l1         & 10   & 2    & 0.86 & 0.86 \\
    invsclaing & l1         & 25   & 2    & 0.89 & 0.89 \\
    invsclaing & l1         & 50   & 2    & 0.89 & 0.89 \\
    invscaling & l1         & 50   & 6    & 0.76 & 0.76 \\
    invscaling & l1         & 25   & 6    & 0.76 & 0.75 \\
    invscaling & l1         & 10   & 6    & 0.72 & 0.72 \\
  \end{tabular}
\end{table}

I ran the experiments with different settings for the learning rate, penalty and number of iterations. The accuracy and f-1 scores for the configurations are listed in tabel~\ref{tab:param}. The parameters influence how the perceptron weights are adjusted. The learning rate can be constant throughout the training or it can decrease in proportion to the length of training or according to local optima. The penalty parameter provides a means of defining what if any regularisation is performed. Regularisation penalises complexity and improves generalisation. The number of iterations determines how many times the learning algorithm goes through the training set.

These three factors learning rate, regularisation and number of iterations are related and it is hard to isolate the influence of any one of them. The results in table~\ref{tab:param} do show that the binary classification is easier in general. For six-way categorisation there a several combinations that perform within 1\% difference from the the highest score of 80\% for both accuracy and f-1. For the binary task there are three different combinations performing the same at 91\%. These combinations have no clear advantageous setting in common.

\subsection{Evaluation}

\begin{table}[ht]\footnotesize
\caption{test cases for generalisation}
\label{tab:generalisation}
\begin{tabular}{ l l l }
test & label & examples \\
\hline
2030s & DATA & 1970s, 60s, '80s, mid-30s \\
Lenovo & ORG & IBM, Microsoft, Apple, Motorola \\
Roosevelt & PERSON & Truman, Nixon, Eisenhower \\
\end{tabular}
\end{table}

\begin{table}[ht]\footnotesize
\caption{counts of topic and sentiment labels}
\label{tab:corpus}
\begin{tabular}{ l r r r r r r }
 & card & date &  gpe & loc & org  & person \\
\hline
  card   & 1178 &    1 &   10 &   2 &  115 &      5 \\
  date   &   46 &  664 &    1 &   0 &  306 &      0 \\
   gpe   &    3 &    1 & 2283 &  50 &  528 &     50 \\
   loc   &    1 &    0 &    6 & 147 &   17 &      6 \\
   org   &   42 &   24 &  232 &  49 & 1476 &    249 \\
person   &   18 &    4 &  107 &  21 &  236 &   1021 \\
\end{tabular}
\end{table}

I checked the classification of unseen names that are similar to ones in the data. Table~\ref{tab:generalisation} show the tests. They were all classified correctly and this indicates that the classifier might generalised well. However the accuracy and f-1 scores show large portion of misclassifications. Table~\ref{tab:corpus} shows the confusion matrix for the six-way classification. Both cardinals and dates are often mistakenly classified as organisations, 9 and 30 percent respectively. The same goes for geopolitical entities and persons, 18 and 17 percent. Organisations on the other hand are mistakenly classified as persons or geopolitical entities, 12 and 11 percent, but not as cardinals or dates, 2 and 1 percent. Persons are also mistakenly classified as geopolitical entities, 7 percent of the persons. The perceptron is a linear classifier and this can explain some of the misclassifications as organisation names occur in many contexts.

\bibliography{eacl2017}
\bibliography{yourbibfile}

\end{document}
